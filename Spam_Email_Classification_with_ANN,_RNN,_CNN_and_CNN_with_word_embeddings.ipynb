{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Email Classification with ANN, RNN,  CNN and CNN with word embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVq4jjeX0mfu"
      },
      "source": [
        "# ***Import Statements for libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTgxBIivD-QE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wordcloud\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding,concatenate, Bidirectional\n",
        "from keras.layers import Dropout, Input, LeakyReLU, Conv1D, GlobalMaxPooling1D,InputLayer, ReLU, LSTM\n",
        "\n",
        "\n",
        "#Utils\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#!pip3 install keras_metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import io\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b-iuoayEJJM"
      },
      "source": [
        "#metrics\n",
        "from sklearn.metrics import f1_score , recall_score, accuracy_score, precision_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Classifiers \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Time and counters\n",
        "from time import perf_counter\n",
        "\n",
        "#grid search of params\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP lib\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import flatten , PorterStemmer, WordNetLemmatizer, FreqDist\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "#Mark Down print\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    # Print with Markdowns    \n",
        "    display(Markdown(string))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV-wylck0vWT"
      },
      "source": [
        "# **Data Test Train Split for hyper-parameter tuning of : KNN and Gradient boosting**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3ZOnSifNw3z"
      },
      "source": [
        "#Test Train Split of Data\n",
        "# load data\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "data = pd.read_csv('drive/MyDrive/SPAM classification deep learning/Dataset/Augmented_emails.csv')\n",
        "\n",
        "#Label\n",
        "data['spam'] = [1 if x == 1.0 else 0 for x in data['spam']]\n",
        "\n",
        "#remove duplicated data\n",
        "print(f\"duplicated entries {data.duplicated().sum()}\\n\")\n",
        "data = data.drop_duplicates()\n",
        "print(f\"duplicated entries {data.duplicated().sum()}\\n\")\n",
        "\n",
        "#remove Null values\n",
        "print(\"Number of null features in the dataset :\")\n",
        "print(f\"{data.isnull().sum()}\\n\")\n",
        "\n",
        "data.dropna(subset=[\"spam\"], inplace=True)\n",
        "\n",
        "print(\"Number of null features in the dataset :\")\n",
        "print(f\"{data.isnull().sum()}\")\n",
        "print()\n",
        "\n",
        "print(f\"shape of the dataset : {data.shape}, Number of rows and columns : {data.shape[0]} , {data.shape[1]}\\n\")\n",
        "\n",
        "#Make a copy of the data set.\n",
        "data_ANN_RNN_CNN = data.copy()\n",
        "\n",
        "#Splitting the data - 80:20 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"X\"] , data[\"spam\"], test_size = 0.2, random_state = 99)\n",
        "print(f\"Training split input: {X_train.shape}\")\n",
        "print(f\"Testing split input : {X_test.shape}\")\n",
        "print(f\"Training split class: {y_train.shape}\")\n",
        "print(f\"Testing split class : {y_test.shape}\")\n",
        "\n",
        "print(X_train.dtypes)\n",
        "print(X_test.dtypes)\n",
        "print(y_train.dtypes)\n",
        "print(y_test.dtypes)\n",
        "\n",
        "#TF IDF - vectorisation of data , feature extraction\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_vect = tfidf.fit_transform(X_train.values.astype('U'))\n",
        "X_test_vect = tfidf.transform(X_test.values.astype('U'))\n",
        "\n",
        "# Get feature names in the vector\n",
        "#tfidf.get_feature_names()\n",
        "\n",
        "X_train_vect.toarray()\n",
        "print(f\"Training data shape : {X_train_vect.shape}\")\n",
        "X_test_vect.toarray()\n",
        "print(f\"Testing data shape : {X_test_vect.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Ei7jc30__l"
      },
      "source": [
        "# ***Helper function to show Confusion diagram and ROC-AUC ***\n",
        "\n",
        "### ROC-AUC : Compute Area Under the Curve (AUC) using the trapezoidal rule. This is a general function, given points on a curve. For computing the area under the ROC-curve.\n",
        "\n",
        "### AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QChdTxgUOcFn"
      },
      "source": [
        "def get_confusion_matrix_heatmap(y_test,y_pred,fName):\n",
        "    # Confusion Matrix\n",
        "    # sklearn builtin function to calculate confusion matrix values using true labels and predictions\n",
        "    CF = confusion_matrix(y_test,y_pred.round())\n",
        "    # list of labels that will be displayed on the image boxes\n",
        "    labels = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    # list of all possible label values\n",
        "    categories = ['Spam', 'Ham']\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    # count total values present in each cell of the matrix\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in CF.flatten()]\n",
        "    # count percentage of total values present in each cell of the matrix\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in CF.flatten()/np.sum(CF)]\n",
        "    # group the labels to plot in graph\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\"for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "    # reshape true label values according to the requirement\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    # declare graph using heatmap function\n",
        "    heatmap=sns.heatmap(CF, annot=labels, fmt='', cmap='Blues')\n",
        "    # plot confusion matrix\n",
        "    fig = heatmap.get_figure()\n",
        "    # save confusion matrix as image in results folder\n",
        "    fig.savefig('drive/MyDrive/SPAM classification deep learning/heatmaps/'+fName)\n",
        "    # display confusion matrix as numeric values\n",
        "    print(CF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZXxXMxdOdie"
      },
      "source": [
        "def ROC_AUC(y_test, y_pred, fname):\n",
        "    # evluate true positive rate and false positive rate using sklearn builtin function\n",
        "    lr_fpr, lr_tpr, _ = roc_curve(y_test, y_pred)\n",
        "    # find area under curve score\n",
        "    lr_auc = auc(lr_fpr, lr_tpr)\n",
        "\n",
        "    # display auc score\n",
        "    print(\"AUC:\", lr_auc)\n",
        "    # plot linear line with no learning\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # plot tpr and fpr ratio\n",
        "    plt.plot(lr_fpr, lr_tpr, marker='.', label='lr (auc = %0.3f)' % lr_auc)\n",
        "    # assign labels\n",
        "    plt.xlabel('False positive rate')\n",
        "    plt.ylabel('True positive rate')\n",
        "    plt.title('Receiver Operating Characterisics')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(f\"drive/MyDrive/SPAM classification deep learning/Visuals/{fname}\")\n",
        "    return lr_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-y6Dkc2JJE"
      },
      "source": [
        "# KNN - K nearest neighbours\n",
        "\n",
        "# **Hyper parameter tuning : **\n",
        "1.   leaf size\n",
        "2.   n_neighbours\n",
        "3.   P\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7npUCE9OiKt"
      },
      "source": [
        "\"\"\"\n",
        "KNN()\n",
        "\"\"\"\n",
        "# Find the best hyperparameter with GridSearchCV\n",
        "# Exhaustive search over specified parameter values for an estimator.\n",
        "\n",
        "#List Hyperparameters that we want to tune.\n",
        "leaf_size = [5,10,15,20,25,30]\n",
        "n_neighbors = [5,10,15,20,25,30]\n",
        "p=[1,2]\n",
        "\n",
        "# #Convert to dictionary\n",
        "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
        "\n",
        "# #Use GridSearch\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid=hyperparameters, cv=10)\n",
        "# #Fit the model\n",
        "grid.fit(X_train_vect,y_train)\n",
        "\n",
        "# # Create a DataFrame with the best Hyperparameters\n",
        "KNN_hyperparameter_tuned = pd.DataFrame(grid.cv_results_)[['params','mean_test_score']]\\\n",
        "                               .sort_values(by=\"mean_test_score\", ascending=False)\n",
        "\n",
        "KNN_hyperparameter_tuned.to_csv(\"drive/MyDrive/SPAM classification deep learning/Visuals/KNN_hyperparameters.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKD-mjsEcWBI"
      },
      "source": [
        "# Plot grid Seach results\n",
        "# scores = [x[1] for x in grid.grid_scores_]\n",
        "# scores = np.array(scores).reshape(len(leaf_size), len(n_neighbors))\n",
        "\n",
        "# for ind, i in enumerate(leaf_size):\n",
        "#     plt.plot(n_neighbors, scores[ind], label='C: ' + str(i))\n",
        "# plt.legend()\n",
        "# plt.xlabel('n neighbors')\n",
        "# plt.ylabel('Mean score')\n",
        "# plt.show()\n",
        "# plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/KNN_Grid_search.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6GGe84QOpzs"
      },
      "source": [
        "\n",
        "# Display the best hyperparameters\n",
        "grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2eaxQBlOvix"
      },
      "source": [
        "leaf_size, n_neighbors, p = grid.best_params_['leaf_size'], grid.best_params_['n_neighbors'], grid.best_params_['p']\n",
        "model = KNeighborsClassifier(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
        "\n",
        "model.fit(X_train_vect,y_train)\n",
        "y_pred = model.predict(X_test_vect)\n",
        "\n",
        "printmd(f'## Accuracy: {round(accuracy_score(y_test,y_pred),3)*100}%\\n')\n",
        "\n",
        "categories = ['Ham', 'Spam']\n",
        "KNN_report = classification_report(y_test,y_pred,target_names=categories)\n",
        "print(KNN_report)\n",
        "\n",
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(y_test, y_pred, \"KNN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3ZZ3CwtIzUz"
      },
      "source": [
        "#AUC\n",
        "lr_auc_knn_tuned = ROC_AUC(y_test, y_pred, \"AUC_KNN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxZU2ywl2mlk"
      },
      "source": [
        "# ***Gradient Boosting : ***\n",
        "### Hyper parameters : \n",
        "\n",
        "\n",
        "1.   Learning rate\n",
        "2.   n_estimators\n",
        "3.   max depth\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsjQVUeGPDx_"
      },
      "source": [
        "\"\"\"\n",
        "GradientBoostingClassifier()\n",
        "\"\"\"\n",
        "# Find the best hyperparameter with GridSearchCV\n",
        "# Exhaustive search over specified parameter values for an estimator.\n",
        "\n",
        "# List Hyperparameters that we want to tune.\n",
        "\n",
        "hyperparameters = {\n",
        "    \"n_estimators\":[5,50,100,150],\n",
        "    \"max_depth\":[1,3,5,7],\n",
        "    \"learning_rate\":[0.01,0.1,1]\n",
        "}\n",
        "start  = perf_counter()\n",
        "#Use GridSearch\n",
        "grid = GridSearchCV(GradientBoostingClassifier(), param_grid=hyperparameters, cv=5)\n",
        "#Fit the model\n",
        "grid.fit(X_train_vect,y_train)\n",
        "\n",
        "end = perf_counter() - start\n",
        "\n",
        "print(f\"Time taken : {end} sec's\")\n",
        "\n",
        "# Create a DataFrame with the best Hyperparameters\n",
        "GB_hyperparameter_tuned = pd.DataFrame(grid.cv_results_)[['params','mean_test_score']]\\\n",
        "                               .sort_values(by=\"mean_test_score\", ascending=False)\n",
        "GB_hyperparameter_tuned.to_csv(\"drive/MyDrive/SPAM classification deep learning/Visuals/Gradient_boosting_hyperparameters.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtW145D_dd5E"
      },
      "source": [
        "#Plot grid Seach results\n",
        "# scores = [x[1] for x in grid.grid_scores_]\n",
        "# scores = np.array(scores).reshape(len(leaf_size), len(n_neighbors))\n",
        "\n",
        "# for ind, i in enumerate(leaf_size):\n",
        "#     plt.plot(n_neighbors, scores[ind], label='C: ' + str(i))\n",
        "# plt.legend()\n",
        "# plt.xlabel('n neighbors')\n",
        "# plt.ylabel('Mean score')\n",
        "# plt.show()\n",
        "# plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/GradientBoosting_Grid_search.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4JdFaSaPMgJ"
      },
      "source": [
        "grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6mk9NmtPP0j"
      },
      "source": [
        "n_estimators, max_depth, learning_rate = grid.best_params_['n_estimators'], grid.best_params_['max_depth'], grid.best_params_['learning_rate']\n",
        "model = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
        "\n",
        "model.fit(X_train_vect,y_train)\n",
        "y_pred = model.predict(X_test_vect)\n",
        "\n",
        "printmd(f'## Accuracy: {round(accuracy_score(y_test,y_pred),3)*100}%\\n')\n",
        "\n",
        "categories = ['Ham', 'Spam']\n",
        "GB_report = classification_report(y_test,y_pred,target_names=categories)\n",
        "print(GB_report)\n",
        "\n",
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(y_test, y_pred, \"Gradient_boosting.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQKyUuEkI8xU"
      },
      "source": [
        "#AUC\n",
        "lr_auc_GradientBoosting_tuned = ROC_AUC(y_test, y_pred, \"AUC_Gradient_Boosting.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPZktPM10WxJ"
      },
      "source": [
        "# Data Test Train Split for DEEP neural Networks : CNN , ANN, RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1QkPFHO4Hu8"
      },
      "source": [
        "# ANN RNN-LSTM CNN \n",
        "#Splitting the data - 80:20 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_ANN_RNN_CNN[\"X\"] , data_ANN_RNN_CNN[\"spam\"], test_size = 0.2, random_state = 99)\n",
        "print(f\"Training split input: {X_train.shape}\")\n",
        "print(f\"Testing split input : {X_test.shape}\")\n",
        "print(f\"Training split class: {y_train.shape}\")\n",
        "print(f\"Testing split class : {y_test.shape}\")\n",
        "\n",
        "maxLen = len(max(X_train.astype(\"U\"), key=len).split())\n",
        "print(f\"Max length of the sentence in corpus : {maxLen}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJrpZ4yl4Q_5"
      },
      "source": [
        "print(f\"Type of X_train : {type(X_train)}\")\n",
        "print(f\"Training sample shape: \\n{X_train.shape}\\n\")\n",
        "x_train = np.array(X_train.copy())\n",
        "x_train = x_train.astype(str)\n",
        "print(f\"Type of X_train : {type(x_train)} with dtypes : {x_train.dtype}\")\n",
        "print(f\"First training sample: \\n{x_train[0]}\\n\")\n",
        "print(f\"Training sample shape: \\n{x_train.shape}\\n\\n\")\n",
        "\n",
        "print(f\"Type of X_test : {type(X_test)}\")\n",
        "print(f\"First testing sample shape: \\n{X_test.shape}\\n\\n\")\n",
        "x_test = np.array(X_test.copy())\n",
        "x_test= x_test.astype(str)\n",
        "print(f\"Type of X_test : {type(x_test)} with dtypes : {x_test.dtype}\")\n",
        "print(f\"First testing sample: \\n{x_test[0]}\\n\\n\")\n",
        "print(f\"Testing sample shape: \\n{x_test.shape}\\n\\n\")\n",
        "\n",
        "print(f\"Type of y_train : {type(y_train)}\")\n",
        "print(f\"First training label shape: \\n{y_train.shape}\\n\\n\")\n",
        "Y_train = np.array(y_train.copy())\n",
        "print(f\"Type of y_train : {type(Y_train)}\")\n",
        "print(f\"First training label : \\n{Y_train[0]}\\n\\n\")\n",
        "print(f\"Training label shape: \\n{Y_train.shape}\\n\\n\")\n",
        "\n",
        "print(f\"Type of y_test : {type(y_test)}\")\n",
        "print(f\"First testing label shape: \\n{y_test.shape}\\n\\n\")\n",
        "Y_test = np.array(y_test.copy())\n",
        "print(f\"Type of y_test : {type(Y_test)}\")\n",
        "print(f\"First testing label : \\n{Y_test[0]}\\n\\n\")\n",
        "print(f\"Testing label shape: \\n{Y_test.shape}\\n\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQFDLdR53FDR"
      },
      "source": [
        "# ***Data set Processing***\n",
        "1. Vocabulary creation \n",
        "2. Tokenization\n",
        "3. Text to sequence / word to vector\n",
        "4. Padding to max length\n",
        "5. Implement sequence/index to word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tb6EQVMdbVx"
      },
      "source": [
        "\"\"\"\n",
        "Input text Processing\n",
        "steps : \n",
        "\n",
        "1. Vocabulary creation \n",
        "2. Tokenization\n",
        "3. Text to sequence / word to vector\n",
        "4. Padding to max length\n",
        "5. Implement sequence/index to word\n",
        "\"\"\"\n",
        "\n",
        "# reference : https://www.kaggle.com/anirudhchandnani/ann-vs-lstm-vs-bi-lstm-on-nlp\n",
        "#ANN\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "tokenizer.word_index.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cfnsOnN70zT"
      },
      "source": [
        "# Index to word Function dict\n",
        "idx_to_word = dict(map(reversed, tokenizer.word_index.items()))\n",
        "print(f\"length of word to index : {len(word_index)}\")\n",
        "print(f\"length of index to word : {len(idx_to_word)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiOAl0x0_5sL"
      },
      "source": [
        "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen = 2000, padding=\"post\")\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen = 2000, padding=\"post\")\n",
        "\n",
        "print(\"Training X Shape:\",X_train.shape)\n",
        "print(\"Testing X Shape:\",X_test.shape)\n",
        "\n",
        "print(f\"Training : {type(x_train)}\")\n",
        "print(f\"Testing: {type(x_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AARV_4fpQzlz"
      },
      "source": [
        "#The first 5 training samples\n",
        "for i in range(5):\n",
        "  print(x_train[i],\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOmk3rl3PzF"
      },
      "source": [
        "# **ANN : Artificial Neural Network : A shallow NN with hidden layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se7rhRO2fFRn"
      },
      "source": [
        "\"\"\"\n",
        "ANN : Artificial Neural Network : A shallow NN with hidden layer.\n",
        "\"\"\"\n",
        "maxlen = 2000\n",
        "ann = Sequential()\n",
        "ann.add(Embedding(input_dim=vocab_size, \n",
        "                           output_dim=100, \n",
        "                           input_length=maxlen))\n",
        "ann.add(GlobalMaxPooling1D())\n",
        "ann.add(Dense(10, activation='relu'))\n",
        "# Adding dropout to prevent overfitting\n",
        "ann.add(Dropout(0.1))\n",
        "ann.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6lr7KdUfPcG"
      },
      "source": [
        "ann.summary()\n",
        "# Visualise the model another way\n",
        "plot_model(\n",
        "    ann,\n",
        "    to_file=\"drive/MyDrive/SPAM classification deep learning/Visuals/ann_1.jpeg\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PvSG9byfQFA"
      },
      "source": [
        "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXAp5MRjfTUA"
      },
      "source": [
        "history = ann.fit(x_train, Y_train, epochs=3, validation_data=(x_test,Y_test), validation_steps = len(x_test), steps_per_epoch= len(x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POuoeVaffVHz"
      },
      "source": [
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n",
        "plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/ann_accuracy_loss.jpeg\")\n",
        "\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = ann.predict(x_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred_ann = np.array(y_pred)\n",
        "\n",
        "test_loss_ann, test_acc_ann = ann.evaluate(x_test, Y_test)\n",
        "test_err_ann = 100 - test_acc_ann*100\n",
        "\n",
        "print(f\"Test Loss:     {test_loss_ann*100} %\")\n",
        "print(f\"Test Accuracy: {test_acc_ann*100}  %\")\n",
        "print(f\"Test error: {test_err_ann}  %\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVXDqpDsFADp"
      },
      "source": [
        "#AUC\n",
        "lr_auc_ann = ROC_AUC(Y_test, y_pred_ann, \"AUC_ANN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHvJAseNdLqy"
      },
      "source": [
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(Y_test, y_pred_ann, \"ANN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWHrZtah56dT"
      },
      "source": [
        "# ANN - TSNE - word Embedding visualisations\n",
        "def Word_Embeddings_visualise_TSNE(model, idx_to_word, fname, lname, lim1, lim2, lim3, lim4, Flag=False):\n",
        "  np.random.seed(1)\n",
        "  print(f\"First sample of the training data set vectorised \\n{x_train[0]}\\n\")\n",
        "  text = [idx_to_word[idx] if idx != 0 else \"<UNK>\" for idx in x_train[0]]\n",
        "  print(f\"First sample of the training data set using index to word : \\n{' '.join(text)}\\n\")\n",
        "\n",
        "  ## Extraction of word Embeddings\n",
        "  word_embeddings = model.get_layer(lname).get_weights()[0]\n",
        "  print('Shape of word_embeddings:', word_embeddings.shape)\n",
        "  \n",
        "  # # Visualizing the word Embeddings\n",
        "  # idx_to_word[0] = \"<PAD>\"\n",
        "  # index = idx_to_word.values()\n",
        "  # Embeddings_ann = pd.DataFrame(word_embeddings, index=index)\n",
        "  # Embeddings_ann.to_csv(f\"drive/MyDrive/SPAM classification deep learning/Visuals/{fname}.csv\")\n",
        "  \n",
        "  # Ploting the word embeddings using TSNE\n",
        "  tsne = TSNE(perplexity=3, n_components=2, init='pca', n_iter=500, method='exact')\n",
        "  np.set_printoptions(suppress=True)\n",
        "  plot_only = 60\n",
        "\n",
        "  T = tsne.fit_transform(word_embeddings[:plot_only, :])\n",
        "  labels = [idx_to_word[i+1] for i in range(plot_only)]\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  if(Flag):\n",
        "    plt.ylim(lim1, lim2)\n",
        "    plt.xlim(lim3, lim4)\n",
        "  plt.scatter(T[:, 0], T[:, 1])\n",
        "  for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "      plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points', ha='right',\n",
        "  va='bottom')\n",
        "  plt.savefig(f\"drive/MyDrive/SPAM classification deep learning/Visuals/{fname}.jpeg\")\n",
        "  # plt.close()\n",
        "  # Embeddings_ann.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWpq4C5vHaWG"
      },
      "source": [
        "# idx_to_word[0]\n",
        "# ANN - TSNE - word Embedding visualisations\n",
        "Word_Embeddings_visualise_TSNE(ann, idx_to_word, \"ann_Embeddings_1\", \"embedding\", -200, 200, -200, 200, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz6mZfLJ3hxI"
      },
      "source": [
        "# **RNN - Recurrent Neural Network \n",
        "Variant used : LSTM : Long short term memory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drHEOXlLg7B1"
      },
      "source": [
        "\"\"\"\n",
        "RNN - Recurrent Neural Network \n",
        "Variant used : LSTM : Long short term memory\n",
        "\"\"\"\n",
        "\n",
        "rnn = Sequential()\n",
        "rnn.add(Embedding(input_dim=vocab_size, \n",
        "                           output_dim=100, \n",
        "                           input_length=2000,\n",
        "                           # Use masking to handle the variable sequence lengths\n",
        "                           mask_zero=True))\n",
        "rnn.add(Dropout(0.2))\n",
        "# rnn.add(Bidirectional(LSTM(128, activation='relu', input_dim=50)))\n",
        "rnn.add(Bidirectional(LSTM(100)))\n",
        "rnn.add(Dense(64, activation='relu'))\n",
        "# Adding dropout to prevent overfitting\n",
        "rnn.add(Dropout(0.2))\n",
        "rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDKNjDkQhN2b"
      },
      "source": [
        "rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_braLcDhQHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a6b7b1-cf8c-49a0-9e6b-dc19c6a7313d"
      },
      "source": [
        "history = rnn.fit(x_train, Y_train, epochs=3, validation_data=(x_test, Y_test), validation_steps = len(x_test), steps_per_epoch= len(x_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            " 420/4396 [=>............................] - ETA: 2:16:09 - loss: 0.4842 - accuracy: 0.8381"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfh_5Jb-6dtp"
      },
      "source": [
        "rnn.summary()\n",
        "plot_model(\n",
        "    rnn,\n",
        "    to_file=\"drive/MyDrive/SPAM classification deep learning/Visuals/rnn_1.jpeg\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrNjRQX8hbC_"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = rnn.predict(x_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "y_pred_rnn = np.array(y_pred)\n",
        "\n",
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(Y_test, y_pred_rnn, \"RNN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnUFYf6bjOOY"
      },
      "source": [
        "#AUC\n",
        "lr_auc_rnn = ROC_AUC(Y_test, y_pred_rnn, \"AUC_RNN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcWrJBK5KHE9"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n",
        "plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/rnn_accuracy_loss.jpeg\")\n",
        "\n",
        "test_loss_rnn, test_acc_rnn = rnn.evaluate(x_test, Y_test)\n",
        "test_err_rnn = 100 - test_acc_rnn*100\n",
        "\n",
        "print(f\"Test Loss:     {test_loss_rnn*100} %\")\n",
        "print(f\"Test Accuracy: {test_acc_rnn*100}  %\")\n",
        "print(f\"Test error: {test_err_rnn}  %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg-Po0tTjXkW"
      },
      "source": [
        "# ANN - TSNE - word Embedding visualisations\n",
        "Word_Embeddings_visualise_TSNE(rnn, idx_to_word, \"rnn_Embeddings_1\", \"embedding_1\", -500, 500, -300, 200, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vewBXd233p_0"
      },
      "source": [
        "# ***CNN without Pretrained Word Embeddings ***\n",
        "## Input : Embedding vector dim : (Embed Size : 100, max Length of sentences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ni5S0gvs_il"
      },
      "source": [
        "\"\"\"\n",
        "CNN without Pretrained Word Embeddings\n",
        "Input : Embedding vector dim : (Embed Size : 100, max Length of sentences)\n",
        "\"\"\"\n",
        "# total vocabualry size of dataset\n",
        "VOCAB_SIZE=vocab_size\n",
        "\n",
        "# maximum length of each sentence/instance in the dataset\n",
        "maxLen=2000\n",
        "\n",
        "# declare keras sequential model\n",
        "cnn = Sequential()\n",
        "\n",
        "# initialize first model layer as embedding layer, here we are not using any pretrained word embedding\n",
        "cnn.add(Embedding(VOCAB_SIZE, 100, input_length=maxLen))\n",
        "\n",
        "''' --------------------\n",
        "- initialize second hidden layer as convolutional layer\n",
        "- number of filters that cnn will return is set to 20\n",
        "- kernel size is set to 3\n",
        "- padding is et to valid, which means no padding will be applied\n",
        "- number of strides is set to default that is 1\n",
        "-------------------- '''\n",
        "cnn.add(Conv1D(activation=\"relu\", filters=20, kernel_size=3, padding=\"valid\"))\n",
        "\n",
        "# we use global max pooling to downsample the features that will simply\n",
        "# select the maximum values from features representation\n",
        "# this layer have no parameters because it just have to select max values no\n",
        "# backpropagation is required\n",
        "cnn.add(GlobalMaxPooling1D())\n",
        "\n",
        "# fully connected layer, usually used to change the dimension of the output\n",
        "# maps all the feature units to mentioned dimension\n",
        "cnn.add(Dense(units=16))\n",
        "\n",
        "# output layer, last layer of model must be fully connected layer that will\n",
        "# map the outputs of previous layer to required output dimension\n",
        "# as we required only 1 unit for output that will return 0 or 1\n",
        "# sigmoid activation function will return value between 0 or 1\n",
        "cnn.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "# initialize the optimizer with learning rate 0.001\n",
        "opt = Adam(learning_rate=0.001)\n",
        "\n",
        "# model compilation where we defince the loss function and optimizer\n",
        "cnn.compile( optimizer=opt, loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# display model layers\n",
        "print(cnn.summary())\n",
        "plot_model(\n",
        "    cnn,\n",
        "    to_file=\"drive/MyDrive/SPAM classification deep learning/Visuals/cnn_1.jpeg\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9x5HtX7mI3W"
      },
      "source": [
        "# start model training\n",
        "history = cnn.fit(x_train, Y_train, epochs=5, validation_steps = len(x_test), steps_per_epoch= len(x_train), validation_data=(x_test, Y_test), verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPA-qLX9mMC8"
      },
      "source": [
        "# evaluate trained model performance on test data\n",
        "cnn.summary()\n",
        "plot_model(\n",
        "    cnn,\n",
        "    to_file=\"drive/MyDrive/SPAM classification deep learning/Visuals/cnn_1.jpeg\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbCRu0Bnmty8"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = cnn.predict(x_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred_cnn = np.array(y_pred)\n",
        "\n",
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(Y_test, y_pred_cnn, \"CNN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmyWPjaam_gq"
      },
      "source": [
        "#AUC\n",
        "lr_auc_cnn = ROC_AUC(Y_test, y_pred_cnn, \"AUC_RNN.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQuUmWyUnCyk"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n",
        "plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/cnn_accuracy_loss.jpeg\")\n",
        "\n",
        "\n",
        "test_loss_cnn, test_acc_cnn = cnn.evaluate(x_test, Y_test)\n",
        "test_err_cnn = 100 - test_acc_cnn*100\n",
        "\n",
        "print(f\"Test Loss:     {test_loss_cnn*100} %\")\n",
        "print(f\"Test Accuracy: {test_acc_cnn*100}  %\")\n",
        "print(f\"Test error: {test_err_cnn}  %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfK6KzqTOqP0"
      },
      "source": [
        "# ANN - TSNE - word Embedding visualisations\n",
        "Word_Embeddings_visualise_TSNE(cnn, idx_to_word, \"cnn_Embeddings_1\", \"embedding_2\", 1,1,1,1, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rsWHNgV4AMA"
      },
      "source": [
        "# **GLOVE : Global vectors for word representation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogXxlv2QnYMq"
      },
      "source": [
        "\"\"\"\n",
        "GLOVE : Global vectors for word representation\n",
        "\n",
        "Definition : \n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations \n",
        "for words. Training is performed on aggregated global word-word co-occurrence statistics from a \n",
        "corpus, and the resulting representations showcase interesting linear substructures of the word \n",
        "vector space.\n",
        "\n",
        "Reference : Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: \n",
        "            Global Vectors for Word Representation.\n",
        "\n",
        "Usage : Using Glove Embedding matrix to convert input data/sample to embedding vectors\n",
        "\"\"\"\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxO00nV8OtY_"
      },
      "source": [
        "\"\"\"\n",
        "Data Preprocessing to be used with glove\n",
        "\n",
        "Tasks : Convert input data/Train data to glove word index\n",
        "\"\"\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('drive/MyDrive/SPAM classification deep learning/Glove/glove.6B.100d.txt')\n",
        "max_length = 2400\n",
        "Embed_size = 100\n",
        "\n",
        "print(len(word_to_index))\n",
        "print(len(index_to_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NPIOMuC4KOi"
      },
      "source": [
        "# Data Pre-Processing \n",
        "1. Helper functions like sentence to index\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5A2QtzkpfI9"
      },
      "source": [
        "def X_to_index(text, word_to_index, max_length):\n",
        "  \n",
        "  vectorised = word_tokenize(text)\n",
        "  vectorised_index = np.zeros((max_length))\n",
        "  # vectorised_index = [word_to_index[w] if w in word_to_index.keys() else  0 for w in vectorised]\n",
        "  i = 0\n",
        "  for w in vectorised:\n",
        "    if w in word_to_index.keys():\n",
        "      vectorised_index[i] = word_to_index[w]\n",
        "    else:\n",
        "      vectorised_index[i] = 0\n",
        "    i += 1\n",
        "    if i >= 2400:\n",
        "      break\n",
        "  return vectorised_index\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "  \"\"\"\n",
        "  Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "  The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "  \n",
        "  Arguments:\n",
        "  X -- array of sentences (strings), of shape (m, 1)\n",
        "  word_to_index -- a dictionary containing the each word mapped to its index\n",
        "  max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "  \n",
        "  Returns:\n",
        "  X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "  \"\"\"\n",
        "  \n",
        "  m = X.shape[0]                                   # number of training examples\n",
        "  # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
        "  X_indices = np.zeros((m, max_len))\n",
        "  \n",
        "  for i in range(m):                               # loop over training examples\n",
        "      \n",
        "      # Convert the ith training sentence in lower case and split is into words. Get a list of words.\n",
        "      sentence_words =X[i].lower().split()\n",
        "      \n",
        "      # Initialize j to 0\n",
        "      j = 0\n",
        "      \n",
        "      # Loop over the words of sentence_words\n",
        "      for w in sentence_words:\n",
        "          # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "          if w in word_to_index.keys():\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "          else:\n",
        "            X_indices[i, j] = 0\n",
        "          \n",
        "          \n",
        "          # Increment j to j + 1\n",
        "          j = j + 1\n",
        "          if j >= 2400:\n",
        "            break\n",
        "  return X_indices\n",
        "\n",
        "data_ANN_RNN_CNN = data_ANN_RNN_CNN.filter(['X','spam'], axis=1)\n",
        "data_ANN_RNN_CNN_index = data_ANN_RNN_CNN[\"X\"].copy()\n",
        "data_ANN_RNN_CNN_index = data_ANN_RNN_CNN_index.astype(str)\n",
        "# data_ANN_RNN_CNN_index = data_ANN_RNN_CNN_index.apply(lambda x: X_to_index(x, word_to_index, max_length))\n",
        "X = sentences_to_indices(data_ANN_RNN_CNN_index, word_to_index, max_length)\n",
        "\n",
        "\n",
        "#Label\n",
        "Y = data_ANN_RNN_CNN['spam'].copy()\n",
        "Y = [1 if x == 1.0 else 0.0 for x in Y]\n",
        "Y = np.asarray(Y).astype('float32') \n",
        "\n",
        "# X.shape\n",
        "print(X.shape)\n",
        "print(type(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6N9UheHspgP"
      },
      "source": [
        "print(type(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJQhA6oMzIgU"
      },
      "source": [
        "print(X.dtype)\n",
        "X = X.astype(\"int32\")\n",
        "print(X.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YmVt_u_zO9P"
      },
      "source": [
        "for i in range(10):\n",
        "  txt = data_ANN_RNN_CNN[\"X\"][i]\n",
        "  idx = X[i]\n",
        "  print(f\"{i}th training sample text : \\n{txt}\\n\")\n",
        "  print(f\"{i}th training sample indexed : \\n{idx}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpbfVg7K4amZ"
      },
      "source": [
        "# ***Data splitting for ANN , RNN , CNN with glove pretrained embeddings model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTfCAaVc8I-p"
      },
      "source": [
        "# type(Y)\n",
        "# #Splitting the data - 80:20 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X , Y, test_size = 0.2, random_state = 99)\n",
        "print(f\"Training split input: {X_train.shape}\")\n",
        "print(f\"Testing split input : {X_test.shape}\")\n",
        "print(f\"Training split class: {y_train.shape}\")\n",
        "print(f\"Testing split class : {y_test.shape}\\n\")\n",
        "\n",
        "print(\"dtypes :\")\n",
        "print(f\"Train : {X_train.dtype}\")\n",
        "print(f\"Test : {X_test.dtype}\")\n",
        "print(f\"Train label : {y_train.dtype}\")\n",
        "print(f\"Test label : {y_test.dtype}\\n\")\n",
        "\n",
        "print(\"Types :\")\n",
        "print(f\"Train : {type(X_train.dtype)}\")\n",
        "print(f\"Test : {type(X_test.dtype)}\")\n",
        "print(f\"Train label : {type(y_train.dtype)}\")\n",
        "print(f\"Test label : {type(y_test.dtype)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO2dvnrb8xiq"
      },
      "source": [
        "for i in range(3):\n",
        "  print(X_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbWaj0JtOthZ"
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding \n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of GloVe word vectors\n",
        "    \n",
        "    # Initialize the embedding matrix as a numpy array of zeros.\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct input and output sizes\n",
        "    # Make it non-trainable.\n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
        "\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
        "    embedding_layer.build((None,)) \n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. The layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4AyHU2Q4ni-"
      },
      "source": [
        "# **Helper Functions to make ANN with Glove Embeddings data **\n",
        "# Complete ANN model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcMpofD2Otl_"
      },
      "source": [
        "def ANN_with_glove(input_shape, word_to_vec_map, word_to_index):\n",
        "  \"\"\"\n",
        "  Function creating the ANN_with_glove model's graph.\n",
        "\n",
        "  Arguments:\n",
        "  input_shape -- shape of the input, usually (max_len,)\n",
        "  word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "  word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "  Returns:\n",
        "  model -- a model instance in Keras\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define sentence_indices as the input of the graph\n",
        "  # It should be of shape input_shape and dtype 'int32'.\n",
        "  sentence_indices = Input(input_shape, dtype='int32')\n",
        "\n",
        "  # Create the embedding layer pretrained with GloVe Vectors\n",
        "  embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "\n",
        "  # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "  embeddings = embedding_layer(sentence_indices)    \n",
        "\n",
        "  # Propagate the embeddings through a max pooling layer with default kernal\n",
        "  X = GlobalMaxPooling1D()(embeddings)\n",
        "\n",
        "  # Propagate X through a Dense layer with relu activation to get back activation of next layer\n",
        "  X = Dense(20, activation='relu')(X)\n",
        "\n",
        "  # Add dropout with a probability of 0.5\n",
        "  X = Dropout(0.5)(X)\n",
        "\n",
        "  # Propagate X through a Dense layer with sigmoid activation to get back activation of next layer\n",
        "  X = Dense(10, activation='relu')(X)\n",
        "\n",
        "  # Add dropout with a probability of 0.5\n",
        "  X = Dropout(0.2)(X)\n",
        "\n",
        "  # Propagate X through a Dense layer with sigmoid activation to get back y_pred\n",
        "  X = Dense(1, activation='sigmoid')(X)\n",
        "  \n",
        "  # # Add a sigmoid activation\n",
        "  # X = Activation('sigmoid')(X)\n",
        "\n",
        "  # Create Model instance which converts sentence_indices into X.\n",
        "  model = Model(inputs=sentence_indices, outputs=X)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTy7dQQA5E5B"
      },
      "source": [
        "# Helper Functions to make RNN with Glove Embeddings data * Complete RNN model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BayQ4wHcOto8"
      },
      "source": [
        "def RNN_with_glove(input_shape, word_to_vec_map, word_to_index):\n",
        "  \"\"\"\n",
        "  Function creating the ANN_with_glove model's graph.\n",
        "\n",
        "  Arguments:\n",
        "  input_shape -- shape of the input, usually (max_len,)\n",
        "  word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "  word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "  Returns:\n",
        "  model -- a model instance in Keras\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define sentence_indices as the input of the graph\n",
        "  # It should be of shape input_shape and dtype 'int32'.\n",
        "  sentence_indices = Input(input_shape, dtype='int32')\n",
        "\n",
        "  # Create the embedding layer pretrained with GloVe Vectors\n",
        "  embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "\n",
        "  # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "  embeddings = embedding_layer(sentence_indices)    \n",
        "\n",
        "  # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "  X = LSTM(128, return_sequences=True)(embeddings)\n",
        "\n",
        "  # Add dropout with a probability of 0.5\n",
        "  X = Dropout(0.5)(X)\n",
        "\n",
        "  # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "  X = LSTM(128, return_sequences=False)(X)\n",
        "\n",
        "  # Add dropout with a probability of 0.5\n",
        "  X = Dropout(0.5)(X)\n",
        "\n",
        "  # Propagate X through a Dense layer with relu activation to get back activation\n",
        "  X = Dense(5, activation='relu')(X)\n",
        "\n",
        "  # Propagate X through a Dense layer with sigmoid activation to get back y_pred\n",
        "  X = Dense(1, activation='sigmoid')(X)\n",
        "\n",
        "  # # Add a sigmoid activation\n",
        "  # X = Activation('sigmoid')(X)\n",
        "\n",
        "  # Create Model instance which converts sentence_indices into X.\n",
        "  model = Model(inputs=sentence_indices, outputs=X)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp7bOttp5Na5"
      },
      "source": [
        "# Helper Functions to make CNN with Glove Embeddings data * Complete CNN model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI0iquI5Otrq"
      },
      "source": [
        "def CNN_with_glove(input_shape, word_to_vec_map, word_to_index):\n",
        "  \"\"\"\n",
        "  Function creating the ANN_with_glove model's graph.\n",
        "\n",
        "  Arguments:\n",
        "  input_shape -- shape of the input, usually (max_len,)\n",
        "  word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "  word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "  Returns:\n",
        "  model -- a model instance in Keras\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define sentence_indices as the input of the graph\n",
        "  # It should be of shape input_shape and dtype 'int32'.\n",
        "  sentence_indices = Input(input_shape, dtype='int32')\n",
        "\n",
        "  # Create the embedding layer pretrained with GloVe Vectors\n",
        "  embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "\n",
        "  # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "  embeddings = embedding_layer(sentence_indices)    \n",
        "\n",
        "  ''' --------------------\n",
        "  - initialize second hidden layer as convolutional layer\n",
        "  - number of filters that cnn will return is set to 20\n",
        "  - kernel size is set to 3\n",
        "  - padding is set to valid, which means no padding will be applied\n",
        "  - number of strides is set to default that is 1\n",
        "  -------------------- '''\n",
        "  X = Conv1D(activation=\"relu\",filters=20, kernel_size=3, padding=\"valid\")(embeddings)\n",
        "  \n",
        "\n",
        "  # Propagate the embeddings through a max pooling layer with default kernal\n",
        "  X = GlobalMaxPooling1D()(X)\n",
        "\n",
        "  # Propagate X through a Dense layer with sigmoid activation to get back activation of next layer\n",
        "  X = Dense(units = 20)(X)\n",
        "\n",
        "  # Add dropout with a probability of 0.5\n",
        "  X = Dropout(0.5)(X)\n",
        "\n",
        "  # Propagate X through a Dense layer with sigmoid activation to get back activation of next layer\n",
        "  X = Dense(10, activation='relu')(X)\n",
        "\n",
        "  # Add dropout with a probability of 0.5\n",
        "  X = Dropout(0.2)(X)\n",
        "\n",
        "  # Propagate X through a Dense layer with sigmoid activation to get back y_pred\n",
        "  X = Dense(1, activation='sigmoid')(X)\n",
        "\n",
        "  # Create Model instance which converts sentence_indices into X.\n",
        "  model = Model(inputs=sentence_indices, outputs=X)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHKUI1Ef5W2Q"
      },
      "source": [
        "# ANN with glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ErEgzvOtua"
      },
      "source": [
        "\"\"\"\n",
        "ANN with glove\n",
        "\"\"\"\n",
        "ann_glove = ANN_with_glove((max_length,), word_to_vec_map, word_to_index)\n",
        "ann_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsI1G6G5Otwv"
      },
      "source": [
        "# display model layers\n",
        "print(ann_glove.summary())\n",
        "plot_model(\n",
        "    ann_glove,\n",
        "    to_file=\"drive/MyDrive/SPAM classification deep learning/Visuals/ann_glove_1.jpeg\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba6M-v08_CDH"
      },
      "source": [
        "#Information about dtypes of layers of the model\n",
        "[print(i.shape, i.dtype) for i in ann_glove.inputs]\n",
        "print(\"\\n\\n\")\n",
        "[print(o.shape, o.dtype) for o in ann_glove.outputs]\n",
        "print(\"\\n\\n\")\n",
        "[print(l.name, l.input_shape, l.dtype) for l in ann_glove.layers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYXcKlOn7zRb"
      },
      "source": [
        "history = ann_glove.fit(X_train, y_train, epochs = 20, validation_steps = len(X_test), steps_per_epoch= len(X_train), validation_data=(X_test, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hez3dBm37zdp"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = ann_glove.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred_ann_glove = np.array(y_pred)\n",
        "\n",
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(y_test, y_pred_ann_glove, \"ANN_glove.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7GrJ3Wv7zhv"
      },
      "source": [
        "#AUC\n",
        "lr_auc_ann_glove = ROC_AUC(Y_test, y_pred_ann_glove, \"AUC_ANN_GLOVE.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khv2R0BD7zkW"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n",
        "plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/ann_glove_accuracy_loss.jpeg\")\n",
        "\n",
        "\n",
        "test_loss_ann_glove, test_acc_ann_glove = ann_glove.evaluate(X_test, y_test)\n",
        "test_err_ann_glove = 100 - test_acc_ann_glove*100\n",
        "\n",
        "print(f\"Test Loss:     {test_loss_ann_glove*100} %\")\n",
        "print(f\"Test Accuracy: {test_acc_ann_glove*100}  %\")\n",
        "print(f\"Test error: {test_err_ann_glove}  %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tv1wr9F7znl"
      },
      "source": [
        "# ANN - TSNE - word Embedding visualisations\n",
        "def Word_Embeddings_visualise_TSNE_glove(model, idx_to_word, fname, lname, lim1, lim2, lim3, lim4, Flag=False):\n",
        "  np.random.seed(1)\n",
        "  print(f\"First sample of the training data set vectorised \\n{X_train[0]}\\n\")\n",
        "  text = [idx_to_word[idx] if idx!=0 else \"<UNK>\" for idx in X_train[0]]\n",
        "  print(f\"First sample of the training data set using index to word : \\n{' '.join(text)}\\n\")\n",
        "\n",
        "  ## Extraction of word Embeddings\n",
        "  word_embeddings = model.get_layer(lname).get_weights()[0]\n",
        "  print('Shape of word_embeddings:', word_embeddings.shape)\n",
        "  # Visualizing the word Embeddings\n",
        "  # Embeddings_model = pd.DataFrame(word_embeddings, index=idx_to_word.values())\n",
        "  # Embeddings_model.to_csv(f\"drive/MyDrive/SPAM classification deep learning/Visuals/{fname}.csv\")\n",
        "  \n",
        "  # Ploting the word embeddings using TSNE\n",
        "  tsne = TSNE(perplexity=3, n_components=2, init='pca', n_iter=500, method='exact')\n",
        "  np.set_printoptions(suppress=True)\n",
        "  plot_only = 60\n",
        "\n",
        "  T = tsne.fit_transform(word_embeddings[:plot_only, :])\n",
        "  labels = [idx_to_word[i+1] for i in range(plot_only)]\n",
        "  plt.figure(figsize=(14, 8))\n",
        "  if(Flag):\n",
        "    plt.ylim(lim1, lim2)\n",
        "    plt.xlim(lim3, lim4)\n",
        "  plt.scatter(T[:, 0], T[:, 1])\n",
        "  for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "      plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points', ha='right',\n",
        "  va='bottom')\n",
        "  plt.savefig(f\"drive/MyDrive/SPAM classification deep learning/Visuals/{fname}.jpeg\")\n",
        "  # plt.close()\n",
        "  # Embeddings_ann.head(10)\n",
        "# ANN - TSNE - word Embedding visualisations\n",
        "Word_Embeddings_visualise_TSNE_glove(ann_glove, index_to_word, \"ann_glove_Embeddings_1\", \"embedding_3\", -200,200,-200,200, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quZlQkd35kdi"
      },
      "source": [
        "# RNN with glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paW_TIxl7zqC"
      },
      "source": [
        "\"\"\"\n",
        "RNN with glove\n",
        "\"\"\"\n",
        "rnn_glove = RNN_with_glove((max_length,), word_to_vec_map, word_to_index)\n",
        "rnn_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNMfAiXfRVeW"
      },
      "source": [
        "# display model layers\n",
        "print(rnn_glove.summary())\n",
        "plot_model(\n",
        "    rnn_glove,\n",
        "    to_file=\"drive/MyDrive/SPAM classification deep learning/Visuals/rnn_glove_1.jpeg\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHm77qY4RVkM"
      },
      "source": [
        "#Information about dtypes of layers of the model\n",
        "[print(i.shape, i.dtype) for i in rnn_glove.inputs]\n",
        "print(\"\\n\\n\")\n",
        "[print(o.shape, o.dtype) for o in rnn_glove.outputs]\n",
        "print(\"\\n\\n\")\n",
        "[print(l.name, l.input_shape, l.dtype) for l in rnn_glove.layers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1X4cqRuRVnQ"
      },
      "source": [
        "history = rnn_glove.fit(X_train, y_train, epochs = 3, validation_steps = len(X_test), steps_per_epoch= len(X_train), validation_data=(X_test, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0h4lXTfRVqp"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = rnn_glove.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred_rnn_glove = np.array(y_pred)\n",
        "\n",
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(y_test, y_pred_rnn_glove, \"RNN_glove.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6a-11GIRVtm"
      },
      "source": [
        "#AUC\n",
        "lr_auc_rnn_glove = ROC_AUC(Y_test, y_pred_rnn_glove, \"AUC_RNN_GLOVE.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnQOd_3rRVxb"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n",
        "plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/rnn_glove_accuracy_loss.jpeg\")\n",
        "\n",
        "\n",
        "test_loss_rnn_glove, test_acc_rnn_glove = rnn_glove.evaluate(X_test, y_test)\n",
        "test_err_rnn_glove = 100 - test_acc_rnn_glove*100\n",
        "\n",
        "print(f\"Test Loss:     {test_loss_rnn_glove*100} %\")\n",
        "print(f\"Test Accuracy: {test_acc_rnn_glove*100}  %\")\n",
        "print(f\"Test error: {test_err_rnn_glove}  %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whlHSsdj7ztR"
      },
      "source": [
        "Word_Embeddings_visualise_TSNE_glove(rnn_glove, index_to_word, \"rnn_glove_Embeddings_1\", \"embedding_4\", -200,200,-200,200, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JVFouyP5y6d"
      },
      "source": [
        "# CNN with glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn_uwWEiTE2i"
      },
      "source": [
        "\"\"\"\n",
        "CNN with glove\n",
        "\"\"\"\n",
        "cnn_glove = CNN_with_glove((max_length,), word_to_vec_map, word_to_index)\n",
        "cnn_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCOduT-7TE2j"
      },
      "source": [
        "# display model layers\n",
        "print(cnn_glove.summary())\n",
        "plot_model(\n",
        "    cnn_glove,\n",
        "    to_file=\"drive/MyDrive/SPAM classification deep learning/Visuals/cnn_glove_1.jpeg\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTs5wFhmTE2j"
      },
      "source": [
        "#Information about dtypes of layers of the model\n",
        "[print(i.shape, i.dtype) for i in cnn_glove.inputs]\n",
        "print(\"\\n\\n\")\n",
        "[print(o.shape, o.dtype) for o in cnn_glove.outputs]\n",
        "print(\"\\n\\n\")\n",
        "[print(l.name, l.input_shape, l.dtype) for l in cnn_glove.layers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z04D9ahTE2k"
      },
      "source": [
        "history = cnn_glove.fit(X_train, y_train, epochs = 5, validation_steps = len(X_test), steps_per_epoch= len(X_train), validation_data=(X_test, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcNwNNA0TE2k"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = cnn_glove.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred_cnn_glove = np.array(y_pred)\n",
        "\n",
        "#confusion matrix\n",
        "get_confusion_matrix_heatmap(y_test, y_pred_cnn_glove, \"CNN_glove.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diOF0sj6TE2k"
      },
      "source": [
        "#AUC\n",
        "lr_auc_cnn_glove = ROC_AUC(Y_test, y_pred_cnn_glove, \"AUC_CNN_GLOVE.jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWvPQlI8TE2k"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n",
        "plt.savefig(\"drive/MyDrive/SPAM classification deep learning/Visuals/cnn_glove_accuracy_loss.jpeg\")\n",
        "\n",
        "\n",
        "test_loss_cnn_glove, test_acc_cnn_glove = cnn_glove.evaluate(X_test, y_test)\n",
        "test_err_cnn_glove = 100 - test_acc_cnn_glove*100\n",
        "\n",
        "print(f\"Test Loss:     {test_loss_cnn_glove*100} %\")\n",
        "print(f\"Test Accuracy: {test_acc_cnn_glove*100}  %\")\n",
        "print(f\"Test error: {test_err_cnn_glove}  %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKidlPqqTE2l"
      },
      "source": [
        "Word_Embeddings_visualise_TSNE_glove(cnn_glove, index_to_word, \"cnn_glove_Embeddings_1\", \"embedding_5\", -200,200,-200,200, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_cCE4G56o5"
      },
      "source": [
        "# Model comparison table with metrics : \n",
        "1. Accuracy\n",
        "2. Loss\n",
        "3. Error\n",
        "4. Precision, Recall, F1 score\n",
        "5. ROC AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj-7QbBiy-Gh"
      },
      "source": [
        "\"\"\"\n",
        "Model comparison table with metrics : \n",
        "1. Accuracy\n",
        "2. Loss\n",
        "3. Error\n",
        "4. Precision, Recall, F1 score\n",
        "5. ROC AUC\n",
        "\"\"\"\n",
        "def get_Metrics(y_test, y_pred, average=\"macro\"):\n",
        "    \n",
        "    lr_fpr, lr_tpr, _ = roc_curve(y_test, y_pred)\n",
        "    # find area under curve score\n",
        "    lr_auc = auc(lr_fpr, lr_tpr)\n",
        "    precision = precision_score(y_test, y_pred, average = average)\n",
        "    recall = recall_score(y_test, y_pred, average = average)\n",
        "    f1_score_ = f1_score(y_test, y_pred, average = average)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    #print(f\"precision : {precision} recall : {recall} f1_score : {f1_score_} accuracy : {accuracy}\")\n",
        "    return precision, recall, f1_score_, accuracy, lr_auc\n",
        "\n",
        "Model_comparison = {\n",
        "    \"Ann without pretrained embeddings\": {\"model\":ann, \"acc\":0, \"loss\" : 0, \"err\":0, \"lr_auc\":0, \"f1_score\":0, \"precision\":0, \"recall\":0, \"y_pred\":0},\n",
        "    \"Rnn without pretrained embeddings\": {\"model\":rnn, \"acc\":0, \"loss\" : 0, \"err\":0, \"lr_auc\":0, \"f1_score\":0, \"precision\":0, \"recall\":0, \"y_pred\":0},\n",
        "    \"Cnn without pretrained embeddings\": {\"model\":cnn, \"acc\":0, \"loss\" : 0, \"err\":0, \"lr_auc\":0, \"f1_score\":0, \"precision\":0, \"recall\":0, \"y_pred\":0},\n",
        "    \"Ann with GLOVE embeddings\": {\"model\":ann_glove, \"acc\":0, \"loss\" : 0, \"err\":0, \"lr_auc\":0, \"f1_score\":0, \"precision\":0, \"recall\":0, \"y_pred\":0},\n",
        "    \"Rnn with GLOVE embeddings\": {\"model\":rnn_glove, \"acc\":0, \"loss\" : 0, \"err\":0, \"lr_auc\":0, \"f1_score\":0, \"precision\":0, \"recall\":0, \"y_pred\":0},\n",
        "    \"Cnn with GLOVE embeddings\": {\"model\":cnn_glove, \"acc\":0, \"loss\" : 0, \"err\":0, \"lr_auc\":0, \"f1_score\":0, \"precision\":0, \"recall\":0, \"y_pred\":0}\n",
        "}\n",
        "\n",
        "for name, model in Model_comparison.items():\n",
        "  if name == \"Ann without pretrained embeddings\":\n",
        "      model[\"y_pred\"]=y_pred_ann\n",
        "      print(type(model[\"y_pred\"]))\n",
        "      print(model[\"y_pred\"].shape)\n",
        "  if name == \"Rnn without pretrained embeddings\":\n",
        "    model[\"y_pred\"]=y_pred_rnn\n",
        "  if name == \"Cnn without pretrained embeddings\":\n",
        "    model[\"y_pred\"]=y_pred_cnn\n",
        "  if name == \"Ann with GLOVE embeddings\":\n",
        "    model[\"y_pred\"]= y_pred_ann_glove\n",
        "  if name == \"Rnn with GLOVE embeddings\":\n",
        "    model[\"y_pred\"]=y_pred_rnn_glove\n",
        "  if name == \"Cnn with GLOVE embeddings\":\n",
        "    model[\"y_pred\"]=y_pred_cnn_glove\n",
        "\n",
        "for name, model in Model_comparison.items():\n",
        "    y_pred = model[\"y_pred\"]\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    precision, recall, f1_score_, accuracy, lr_auc = get_Metrics(Y_test, y_pred)\n",
        "    if name == \"Ann without pretrained embeddings\":\n",
        "      model[\"acc\"]=test_acc_ann\n",
        "      model[\"loss\"]=test_loss_ann\n",
        "      model[\"err\"]=test_err_ann\n",
        "      model[\"lr_auc\"]=lr_auc_ann\n",
        "      model[\"f1_score\"]=precision\n",
        "      model[\"precision\"]=recall\n",
        "      model[\"recall\"]=f1_score_\n",
        "      \n",
        "    if name == \"Rnn without pretrained embeddings\":\n",
        "      model[\"acc\"]=test_acc_rnn\n",
        "      model[\"loss\"]=test_loss_rnn\n",
        "      model[\"err\"]=test_err_rnn\n",
        "      model[\"lr_auc\"]=lr_auc_rnn\n",
        "      model[\"f1_score\"]=precision\n",
        "      model[\"precision\"]=recall\n",
        "      model[\"recall\"]=f1_score_\n",
        "    \n",
        "    if name == \"Cnn without pretrained embeddings\":\n",
        "      model[\"acc\"]=test_acc_cnn\n",
        "      model[\"loss\"]=test_loss_cnn\n",
        "      model[\"err\"]=test_err_cnn\n",
        "      model[\"lr_auc\"]=lr_auc_cnn\n",
        "      model[\"f1_score\"]=precision\n",
        "      model[\"precision\"]=recall\n",
        "      model[\"recall\"]=f1_score_\n",
        "      \n",
        "    if name == \"Ann with GLOVE embeddingss\":\n",
        "      model[\"acc\"]=test_acc_ann_glove\n",
        "      model[\"loss\"]=test_loss_ann_glove\n",
        "      model[\"err\"]=test_err_ann_glove\n",
        "      model[\"lr_auc\"]=lr_auc_ann_glove\n",
        "      model[\"f1_score\"]=precision\n",
        "      model[\"precision\"]=recall\n",
        "      model[\"recall\"]=f1_score_\n",
        "      \n",
        "    if name == \"Rnn with GLOVE embeddings\":\n",
        "      model[\"acc\"]=test_acc_rnn_glove\n",
        "      model[\"loss\"]=test_loss_rnn_glove\n",
        "      model[\"err\"]=test_err_rnn_glove\n",
        "      model[\"lr_auc\"]=lr_auc_rnn_glove\n",
        "      model[\"f1_score\"]=precision\n",
        "      model[\"precision\"]=recall\n",
        "      model[\"recall\"]=f1_score_\n",
        "     \n",
        "    if name == \"Cnn with GLOVE embeddings\":\n",
        "      model[\"acc\"]=test_acc_cnn_glove\n",
        "      model[\"loss\"]=test_loss_cnn_glove\n",
        "      model[\"err\"]=test_err_cnn_glove\n",
        "      model[\"lr_auc\"]=lr_auc_cnn_glove\n",
        "      model[\"f1_score\"]=precision\n",
        "      model[\"precision\"]=recall\n",
        "      model[\"recall\"]=f1_score_\n",
        "    # print(model)\n",
        "\n",
        "models_metrics = []\n",
        "for name, model in Model_comparison.items():\n",
        "  #print(model[\"y_pred\"].shape)\n",
        "  precision, recall, f1_score_,  = model[\"precision\"], model[\"recall\"], model[\"f1_score\"]\n",
        "  accuracy, lr_auc, loss, error = model[\"acc\"], model[\"lr_auc\"], model[\"loss\"], model[\"err\"]\n",
        "  #print(f\"precision : {precision} recall : {recall} f1_score : {f1_score_} accuracy : {accuracy}\")\n",
        "  models_metrics.append([name, precision, recall, f1_score_, accuracy, error, loss, lr_auc])\n",
        "\n",
        "df_metrics = pd.DataFrame(models_metrics)\n",
        "df_metrics.columns = ['Model', 'Precision', 'Recall', 'f1 score', 'Accuracy', 'Error', 'Loss', 'ROC-AUC']\n",
        "df_metrics.sort_values(by = 'Accuracy', ascending = False, inplace=True)\n",
        "df_metrics.reset_index(drop = True, inplace=True)\n",
        "df_metrics\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhTQeG807sbQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}